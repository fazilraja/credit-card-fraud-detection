{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'creditcard.csv'  # Update the file path if needed\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
    "df = data\n",
    "# The function \"len\" counts the number of classes = 1 and saves it as an object \"fraud_records\"\n",
    "fraud_records = len(df[df.Class == 1])\n",
    "\n",
    "# Defines the index for fraud and non-fraud in the lines:\n",
    "fraud_indices = df[df.Class == 1].index\n",
    "not_fraud_indices = df[df.Class == 0].index\n",
    "\n",
    "# Randomly collect equal samples of each type:\n",
    "under_sample_indices = np.random.choice(not_fraud_indices, fraud_records, False)\n",
    "df_undersampled = df.iloc[np.concatenate([fraud_indices, under_sample_indices]),:]\n",
    "X_undersampled = df_undersampled.iloc[:,1:30]\n",
    "Y_undersampled = df_undersampled.Class\n",
    "X_undersampled_train, X_undersampled_test, Y_undersampled_train, Y_undersampled_test = train_test_split(X_undersampled, Y_undersampled, test_size = 0.3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(688, 29)\n",
      "(296, 29)\n",
      "(688,)\n",
      "(296,)\n"
     ]
    }
   ],
   "source": [
    "# check the size of X_undersampled_train and X_undersampled_test\n",
    "print(X_undersampled_train.shape)\n",
    "print(X_undersampled_test.shape)\n",
    "print(Y_undersampled_train.shape)\n",
    "print(Y_undersampled_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/Fazil/Documents/School/FALL 23/dataMining/credit-card-fraud-detection/regression.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Fazil/Documents/School/FALL%2023/dataMining/credit-card-fraud-detection/regression.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# split the data into train and test 80/20\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Fazil/Documents/School/FALL%2023/dataMining/credit-card-fraud-detection/regression.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Fazil/Documents/School/FALL%2023/dataMining/credit-card-fraud-detection/regression.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                                     test_size\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Fazil/Documents/School/FALL%2023/dataMining/credit-card-fraud-detection/regression.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                                     random_state\u001b[39m=\u001b[39;49m\u001b[39m123\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mlp/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mlp/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2617\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2614\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[1;32m   2616\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m-> 2617\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2618\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[1;32m   2619\u001b[0m )\n\u001b[1;32m   2621\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m   2622\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mlp/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2273\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2270\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[1;32m   2272\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2273\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2274\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2275\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2276\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2277\u001b[0m     )\n\u001b[1;32m   2279\u001b[0m \u001b[39mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# split the data into train and test 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        # Clip the input values to avoid overflow in the exponential function\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.num_iterations):\n",
    "            model = np.dot(X, self.weights) + self.bias\n",
    "            predictions = self._sigmoid(model)\n",
    "\n",
    "            dw = (1 / num_samples) * np.dot(X.T, (predictions - y))\n",
    "            db = (1 / num_samples) * np.sum(predictions - y)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        model = np.dot(X, self.weights) + self.bias\n",
    "        predictions = self._sigmoid(model)\n",
    "        return [1 if i > 0.5 else 0 for i in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5067567567567568\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0.0\n",
      "AUC: 0.5\n",
      "Confusion Matrix: [[150   0]\n",
      " [146   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Fazil/opt/anaconda3/envs/mlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "def compute_gradient(X, y, w, b):\n",
    "   gradient_w = np.dot(X.T, (np.dot(X, w) + b - y)) / y.size\n",
    "   gradient_b = np.sum(np.dot(X, w) + b - y) / y.size\n",
    "   return gradient_w, gradient_b\n",
    "\n",
    "def update_parameters(w, b, gradient_w, gradient_b, learning_rate):\n",
    "   w = w - learning_rate * gradient_w\n",
    "   b = b - learning_rate * gradient_b\n",
    "   return w, b\n",
    "\n",
    "def logistic_regression(X, y, w, b, learning_rate, num_iterations):\n",
    "   for i in range(num_iterations):\n",
    "       gradient_w, gradient_b = compute_gradient(X, y, w, b)\n",
    "       w, b = update_parameters(w, b, gradient_w, gradient_b, learning_rate)\n",
    "   return w, b\n",
    "\n",
    "def predict(X, w, b):\n",
    "   return 1 / (1 + np.exp(-(np.dot(X, w) + b)))\n",
    "\n",
    "w = np.zeros(X_undersampled_train.shape[1])\n",
    "b = 0\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "w, b = logistic_regression(X_undersampled_train, Y_undersampled_train, w, b, learning_rate, num_iterations)\n",
    "# predict\n",
    "y_pred = predict(X_undersampled_test, w, b)\n",
    "y_pred = np.where(y_pred > 0.6, 1, 0)\n",
    "\n",
    "# evaluate\n",
    "print('Accuracy:', accuracy_score(Y_undersampled_test, y_pred))\n",
    "print('Precision:', precision_score(Y_undersampled_test, y_pred))\n",
    "print('Recall:', recall_score(Y_undersampled_test, y_pred))\n",
    "print('F1:', f1_score(Y_undersampled_test, y_pred))\n",
    "print('AUC:', roc_auc_score(Y_undersampled_test, y_pred))\n",
    "print('Confusion Matrix:', confusion_matrix(Y_undersampled_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model = LogisticRegression(learning_rate=0.01, n_iterations=1000)\n",
    "model.fit(X_undersampled_train, Y_undersampled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_pred = model.predict(X_undersampled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8783783783783784\n",
      "Precision: 0.9230769230769231\n",
      "Recall: 0.821917808219178\n",
      "F1 Score: 0.8695652173913043\n",
      "ROC AUC Score: 0.8776255707762557\n",
      "Confusion Matrix:\n",
      " [[140  10]\n",
      " [ 26 120]]\n"
     ]
    }
   ],
   "source": [
    "# get the metrics\n",
    "print(\"Accuracy:\", accuracy_score(Y_undersampled_test, y_pred))\n",
    "print(\"Precision:\", precision_score(Y_undersampled_test, y_pred))\n",
    "print(\"Recall:\", recall_score(Y_undersampled_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(Y_undersampled_test, y_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(Y_undersampled_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(Y_undersampled_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn's LogisticRegression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9867886178861789\n",
      "Precision: 1.0\n",
      "Recall: 0.8452380952380952\n",
      "F1 Score: 0.9161290322580644\n",
      "ROC AUC Score: 0.9226190476190477\n"
     ]
    }
   ],
   "source": [
    "# get the metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
