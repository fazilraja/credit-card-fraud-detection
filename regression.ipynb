{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'creditcard.csv'  # Update the file path if needed\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
    "df = data\n",
    "# The function \"len\" counts the number of classes = 1 and saves it as an object \"fraud_records\"\n",
    "fraud_records = len(df[df.Class == 1])\n",
    "\n",
    "# Defines the index for fraud and non-fraud in the lines:\n",
    "fraud_indices = df[df.Class == 1].index\n",
    "not_fraud_indices = df[df.Class == 0].index\n",
    "\n",
    "# Randomly collect equal samples of each type:\n",
    "under_sample_indices = np.random.choice(not_fraud_indices, fraud_records, False)\n",
    "df_undersampled = df.iloc[np.concatenate([fraud_indices, under_sample_indices]),:]\n",
    "X_undersampled = df_undersampled.iloc[:,1:30]\n",
    "Y_undersampled = df_undersampled.Class\n",
    "X_undersampled_train, X_undersampled_test, Y_undersampled_train, Y_undersampled_test = train_test_split(X_undersampled, Y_undersampled, test_size = 0.3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(688, 29)\n",
      "(296, 29)\n",
      "(688,)\n",
      "(296,)\n"
     ]
    }
   ],
   "source": [
    "# check the size of X_undersampled_train and X_undersampled_test\n",
    "print(X_undersampled_train.shape)\n",
    "print(X_undersampled_test.shape)\n",
    "print(Y_undersampled_train.shape)\n",
    "print(Y_undersampled_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        # Clip the input values to avoid overflow in the exponential function\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.num_iterations):\n",
    "            model = np.dot(X, self.weights) + self.bias\n",
    "            predictions = self._sigmoid(model)\n",
    "\n",
    "            dw = (1 / num_samples) * np.dot(X.T, (predictions - y))\n",
    "            db = (1 / num_samples) * np.sum(predictions - y)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        model = np.dot(X, self.weights) + self.bias\n",
    "        predictions = self._sigmoid(model)\n",
    "        return [1 if i > 0.7 else 0 for i in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model = LogisticRegression(learning_rate=0.01, n_iterations=1000)\n",
    "model.fit(X_undersampled_train, Y_undersampled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_pred = model.predict(X_undersampled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7027027027027027\n",
      "Precision: 0.6458333333333334\n",
      "Recall: 0.9810126582278481\n",
      "F1 Score: 0.7788944723618091\n",
      "ROC AUC Score: 0.6825353146211705\n",
      "Confusion Matrix:\n",
      " [[ 53  85]\n",
      " [  3 155]]\n"
     ]
    }
   ],
   "source": [
    "# get the metrics\n",
    "print(\"Accuracy:\", accuracy_score(Y_undersampled_test, y_pred))\n",
    "print(\"Precision:\", precision_score(Y_undersampled_test, y_pred))\n",
    "print(\"Recall:\", recall_score(Y_undersampled_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(Y_undersampled_test, y_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(Y_undersampled_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(Y_undersampled_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('creditcard.csv')\n",
    "\n",
    "# Subsample the data such that 90& of the data is fradulent and 10% is non-fraudulent\n",
    "fraudulent = data[data['Class'] == 1]\n",
    "non_fraudulent = data[data['Class'] == 0]\n",
    "\n",
    "# Randomly sample non-fraudulent transactions\n",
    "non_fraudulent_sample = non_fraudulent.sample(n=len(fraudulent)*9, random_state=42)\n",
    "\n",
    "# Combine the fraudulent and non-fraudulent samples\n",
    "subsample = pd.concat([fraudulent, non_fraudulent_sample])\n",
    "\n",
    "# Split the data into features and target\n",
    "X = subsample.drop(['Class','Time'], axis=1)\n",
    "y = subsample['Class'].values\n",
    "\n",
    "# Split into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.9, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9677055103884372\n",
      "Precision: 0.9875389408099688\n",
      "Recall: 0.6951754385964912\n",
      "F1 Score: 0.8159588159588159\n",
      "ROC AUC Score: 0.8470841946255366\n",
      "Confusion Matrix:\n",
      " [[3968    4]\n",
      " [ 139  317]]\n"
     ]
    }
   ],
   "source": [
    "# get the metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
